{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from alloprompt import Prompt\n",
    "\n",
    "client_or = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from AlloLLMEval import PromptExecutorBase\n",
    "\n",
    "class SentimentExecutor(PromptExecutorBase[str, str]):\n",
    "    def __init__(self):\n",
    "        super().__init__(config_schema={\n",
    "            \"model\": str,\n",
    "            \"temperature\": float\n",
    "        })\n",
    "        \n",
    "    def execute(self, input_data: str, config: Dict[str, Any]) -> str:\n",
    "        model_output = client_or.chat.completions.create(\n",
    "            model=config[\"model\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sentiment classifier. You are given a movie review and you need to classify it as positive, negative or neutral. You must only return the sentiment.\"},\n",
    "                {\"role\": \"user\", \"content\": input_data}\n",
    "            ]\n",
    "        ).choices[0].message.content.lower()\n",
    "        if \"positive\" in model_output:\n",
    "            return \"positive\"\n",
    "        elif \"negative\" in model_output:\n",
    "            return \"negative\"\n",
    "        return \"neutral\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "from AlloLLMEval.core.metrics import MetricEvaluatorBase, MetricOutput, MetricStatus\n",
    "\n",
    "class StrictGroundTruthEvaluator(MetricEvaluatorBase):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        executor: PromptExecutorBase,\n",
    "        input_data: Any,\n",
    "        base_output: Any,\n",
    "        executor_config: Dict[str, Any],\n",
    "        test_config: Dict[str, Any],\n",
    "        evaluation_params: Dict[str, Any]\n",
    "    ) -> MetricOutput:\n",
    "        ground_truth = evaluation_params.get(\"ground_truth\")\n",
    "        if not ground_truth:\n",
    "            raise ValueError(\"Ground truth must be provided in evaluation_params\")\n",
    "        \n",
    "        score = 1.0 if base_output == ground_truth else 0.0\n",
    "        status = MetricStatus.PASSED if score == 1.0 else MetricStatus.FAILED\n",
    "        \n",
    "        return MetricOutput(\n",
    "            score=score,\n",
    "            status=status,\n",
    "            visualization=None,\n",
    "            details={\n",
    "                \"expected\": ground_truth,\n",
    "                \"received\": base_output\n",
    "            },\n",
    "            threshold={\"min_score\": 1.0}\n",
    "        )\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "from AlloLLMEval.core.metrics import MetricEvaluatorBase, MetricOutput, MetricStatus\n",
    "\n",
    "class StabilityEvaluator(MetricEvaluatorBase):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        executor: PromptExecutorBase,\n",
    "        input_data: Any,\n",
    "        base_output: Any,\n",
    "        executor_config: Dict[str, Any],\n",
    "        test_config: Dict[str, Any],\n",
    "        evaluation_params: Dict[str, Any]\n",
    "    ) -> MetricOutput:\n",
    "        num_runs = test_config.get(\"num_runs\", 5)\n",
    "        outputs: List[Any] = [base_output]\n",
    "        \n",
    "        # Run multiple times with temperature=1\n",
    "        high_temp_config = dict(executor_config)\n",
    "        high_temp_config[\"temperature\"] = 1.0\n",
    "        \n",
    "        for _ in range(num_runs - 1):\n",
    "            output = executor.execute(input_data, high_temp_config)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Calculate how many outputs are the same\n",
    "        matching_outputs = sum(1 for out in outputs if out == base_output)\n",
    "        stability_score = matching_outputs / len(outputs)\n",
    "        \n",
    "        return MetricOutput(\n",
    "            score=stability_score,\n",
    "            status=MetricStatus.PASSED if stability_score >= 0.6 else MetricStatus.FAILED,\n",
    "            visualization=None,\n",
    "            details={\n",
    "                \"outputs\": outputs,\n",
    "                \"unique_outputs\": list(set(outputs))\n",
    "            },\n",
    "            threshold={\"min_stability\": 0.6}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Test Results:\n",
      "\n",
      "Review: The movie was fantastic! Great acting and storyline.\n",
      "Expected: positive\n",
      "Received: positive\n",
      "Score: 1.0\n",
      "\n",
      "Review: Terrible waste of time and money. Awful plot.\n",
      "Expected: negative\n",
      "Received: negative\n",
      "Score: 1.0\n",
      "\n",
      "Review: It was okay, nothing special but not bad either.\n",
      "Expected: neutral\n",
      "Received: neutral\n",
      "Score: 1.0\n",
      "\n",
      "Review: Best film I've seen all year! A masterpiece!\n",
      "Expected: positive\n",
      "Received: positive\n",
      "Score: 1.0\n",
      "\n",
      "Review: I fell asleep halfway through. Very boring.\n",
      "Expected: negative\n",
      "Received: negative\n",
      "Score: 1.0\n",
      "\n",
      "Stability Test Results:\n",
      "\n",
      "Review: The movie was fantastic! Great acting and storyline.\n",
      "Stability Score: 1.0\n",
      "Unique outputs: ['positive']\n",
      "\n",
      "Review: Terrible waste of time and money. Awful plot.\n",
      "Stability Score: 1.0\n",
      "Unique outputs: ['negative']\n",
      "\n",
      "Review: It was okay, nothing special but not bad either.\n",
      "Stability Score: 1.0\n",
      "Unique outputs: ['neutral']\n",
      "\n",
      "Review: Best film I've seen all year! A masterpiece!\n",
      "Stability Score: 1.0\n",
      "Unique outputs: ['positive']\n",
      "\n",
      "Review: I fell asleep halfway through. Very boring.\n",
      "Stability Score: 1.0\n",
      "Unique outputs: ['negative']\n"
     ]
    }
   ],
   "source": [
    "from AlloLLMEval import TestRunner, TestConfig\n",
    "\n",
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    (\"The movie was fantastic! Great acting and storyline.\", \"positive\"),\n",
    "    (\"Terrible waste of time and money. Awful plot.\", \"negative\"), \n",
    "    (\"It was okay, nothing special but not bad either.\", \"neutral\"),\n",
    "    (\"Best film I've seen all year! A masterpiece!\", \"positive\"),\n",
    "    (\"I fell asleep halfway through. Very boring.\", \"negative\")\n",
    "]\n",
    "\n",
    "# Initialize components\n",
    "executor = SentimentExecutor()\n",
    "ground_truth_evaluator = StrictGroundTruthEvaluator({})\n",
    "stability_evaluator = StabilityEvaluator({})\n",
    "\n",
    "# Ground truth test\n",
    "ground_truth_test = TestRunner(\n",
    "    executor=executor,\n",
    "    evaluator=ground_truth_evaluator,\n",
    "    config=TestConfig(\n",
    "        executor_config={\"model\": \"gpt-4\", \"temperature\": 0.0},\n",
    "        metric_config={}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run ground truth tests\n",
    "print(\"Ground Truth Test Results:\")\n",
    "for review, ground_truth in reviews:\n",
    "    gt_result = ground_truth_test.run(\n",
    "        input_data=review,\n",
    "        evaluation_params={\"ground_truth\": ground_truth}\n",
    "    )\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Expected: {ground_truth}\")\n",
    "    print(f\"Received: {gt_result.executor_output}\")\n",
    "    print(f\"Score: {gt_result.metric_output.score}\")\n",
    "\n",
    "# Stability test\n",
    "stability_test = TestRunner(\n",
    "    executor=executor,\n",
    "    evaluator=stability_evaluator,\n",
    "    config=TestConfig(\n",
    "        executor_config={\"model\": \"gpt-4\", \"temperature\": 1},\n",
    "        metric_config={\"num_runs\": 5}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run stability tests\n",
    "print(\"\\nStability Test Results:\")\n",
    "for review, _ in reviews:\n",
    "    stability_result = stability_test.run(\n",
    "        input_data=review\n",
    "    )\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Stability Score: {stability_result.metric_output.score}\")\n",
    "    print(f\"Unique outputs: {stability_result.metric_output.details['unique_outputs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
